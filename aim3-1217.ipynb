{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import (negative_sampling, remove_self_loops,\n",
    "                                   add_self_loops)\n",
    "\n",
    "\n",
    "EPS = 1e-15\n",
    "MAX_LOGSTD = 10\n",
    "\n",
    "\n",
    "class InnerProductDecoder(torch.nn.Module):\n",
    "    r\"\"\"The inner product decoder from the `\"Variational Graph Auto-Encoders\"\n",
    "    <https://arxiv.org/abs/1611.07308>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\sigma(\\mathbf{Z}\\mathbf{Z}^{\\top})\n",
    "\n",
    "    where :math:`\\mathbf{Z} \\in \\mathbb{R}^{N \\times d}` denotes the latent\n",
    "    space produced by the encoder.\"\"\"\n",
    "    def forward(self, z, edge_index, sigmoid=True):\n",
    "        r\"\"\"Decodes the latent variables :obj:`z` into edge probabilities for\n",
    "        the given node-pairs :obj:`edge_index`.\n",
    "\n",
    "        Args:\n",
    "            z (Tensor): The latent space :math:`\\mathbf{Z}`.\n",
    "            sigmoid (bool, optional): If set to :obj:`False`, does not apply\n",
    "                the logistic sigmoid function to the output.\n",
    "                (default: :obj:`True`)\n",
    "        \"\"\"\n",
    "        value = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=1)\n",
    "        return torch.sigmoid(value) if sigmoid else value\n",
    "\n",
    "\n",
    "    def forward_all(self, z, sigmoid=True):\n",
    "        r\"\"\"Decodes the latent variables :obj:`z` into a probabilistic dense\n",
    "        adjacency matrix.\n",
    "\n",
    "        Args:\n",
    "            z (Tensor): The latent space :math:`\\mathbf{Z}`.\n",
    "            sigmoid (bool, optional): If set to :obj:`False`, does not apply\n",
    "                the logistic sigmoid function to the output.\n",
    "                (default: :obj:`True`)\n",
    "        \"\"\"\n",
    "        adj = torch.matmul(z, z.t())\n",
    "        return torch.sigmoid(adj) if sigmoid else adj\n",
    "\n",
    "\n",
    "\n",
    "class GAE(torch.nn.Module):\n",
    "    r\"\"\"The Graph Auto-Encoder model from the\n",
    "    `\"Variational Graph Auto-Encoders\" <https://arxiv.org/abs/1611.07308>`_\n",
    "    paper based on user-defined encoder and decoder models.\n",
    "\n",
    "    Args:\n",
    "        encoder (Module): The encoder module.\n",
    "        decoder (Module, optional): The decoder module. If set to :obj:`None`,\n",
    "            will default to the\n",
    "            :class:`torch_geometric.nn.models.InnerProductDecoder`.\n",
    "            (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder=None):\n",
    "        super(GAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = InnerProductDecoder() if decoder is None else decoder\n",
    "        GAE.reset_parameters(self)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.encoder)\n",
    "        reset(self.decoder)\n",
    "\n",
    "\n",
    "    def encode(self, *args, **kwargs):\n",
    "        r\"\"\"Runs the encoder and computes node-wise latent variables.\"\"\"\n",
    "        return self.encoder(*args, **kwargs)\n",
    "\n",
    "\n",
    "    def decode(self, *args, **kwargs):\n",
    "        r\"\"\"Runs the decoder and computes edge probabilities.\"\"\"\n",
    "        return self.decoder(*args, **kwargs)\n",
    "\n",
    "\n",
    "    def recon_loss(self, z, pos_edge_index, neg_edge_index=None):\n",
    "        r\"\"\"Given latent variables :obj:`z`, computes the binary cross\n",
    "        entropy loss for positive edges :obj:`pos_edge_index` and negative\n",
    "        sampled edges.\n",
    "\n",
    "        Args:\n",
    "            z (Tensor): The latent space :math:`\\mathbf{Z}`.\n",
    "            pos_edge_index (LongTensor): The positive edges to train against.\n",
    "            neg_edge_index (LongTensor, optional): The negative edges to train\n",
    "                against. If not given, uses negative sampling to calculate\n",
    "                negative edges. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "\n",
    "        pos_loss = -torch.log(\n",
    "            self.decoder(z, pos_edge_index, sigmoid=True) + EPS).mean()\n",
    "\n",
    "        # Do not include self-loops in negative samples\n",
    "        pos_edge_index, _ = remove_self_loops(pos_edge_index)\n",
    "        pos_edge_index, _ = add_self_loops(pos_edge_index)\n",
    "        if neg_edge_index is None:\n",
    "            neg_edge_index = negative_sampling(pos_edge_index, z.size(0))\n",
    "        neg_loss = -torch.log(1 -\n",
    "                              self.decoder(z, neg_edge_index, sigmoid=True) +\n",
    "                              EPS).mean()\n",
    "\n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "\n",
    "    def test(self, z, pos_edge_index, neg_edge_index):\n",
    "        r\"\"\"Given latent variables :obj:`z`, positive edges\n",
    "        :obj:`pos_edge_index` and negative edges :obj:`neg_edge_index`,\n",
    "        computes area under the ROC curve (AUC) and average precision (AP)\n",
    "        scores.\n",
    "\n",
    "        Args:\n",
    "            z (Tensor): The latent space :math:`\\mathbf{Z}`.\n",
    "            pos_edge_index (LongTensor): The positive edges to evaluate\n",
    "                against.\n",
    "            neg_edge_index (LongTensor): The negative edges to evaluate\n",
    "                against.\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "        pos_y = z.new_ones(pos_edge_index.size(1))\n",
    "        neg_y = z.new_zeros(neg_edge_index.size(1))\n",
    "        y = torch.cat([pos_y, neg_y], dim=0)\n",
    "\n",
    "        pos_pred = self.decoder(z, pos_edge_index, sigmoid=True)\n",
    "        neg_pred = self.decoder(z, neg_edge_index, sigmoid=True)\n",
    "        pred = torch.cat([pos_pred, neg_pred], dim=0)\n",
    "        \n",
    "        y, pred = y.detach().cpu().numpy(), pred.detach().cpu().numpy()\n",
    "\n",
    "        return roc_auc_score(y, pred), average_precision_score(y, pred)\n",
    "\n",
    "\n",
    "\n",
    "class VGAE(GAE):\n",
    "    r\"\"\"The Variational Graph Auto-Encoder model from the\n",
    "    `\"Variational Graph Auto-Encoders\" <https://arxiv.org/abs/1611.07308>`_\n",
    "    paper.\n",
    "\n",
    "    Args:\n",
    "        encoder (Module): The encoder module to compute :math:`\\mu` and\n",
    "            :math:`\\log\\sigma^2`.\n",
    "        decoder (Module, optional): The decoder module. If set to :obj:`None`,\n",
    "            will default to the\n",
    "            :class:`torch_geometric.nn.models.InnerProductDecoder`.\n",
    "            (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder=None):\n",
    "        super(VGAE, self).__init__(encoder, decoder)\n",
    "\n",
    "    def reparametrize(self, mu, logstd):\n",
    "        if self.training:\n",
    "            return mu + torch.randn_like(logstd) * torch.exp(logstd)\n",
    "        else:\n",
    "            return mu\n",
    "    \n",
    "    def encode(self, *args, **kwargs):\n",
    "        \"\"\"\"\"\"\n",
    "        self.__mu__, self.__logstd__ = self.encoder(*args, **kwargs)\n",
    "        self.__logstd__ = self.__logstd__.clamp(max=MAX_LOGSTD)\n",
    "        z = self.reparametrize(self.__mu__, self.__logstd__)\n",
    "        return z\n",
    "\n",
    "    def kl_loss(self, mu=None, logstd=None):\n",
    "        r\"\"\"Computes the KL loss, either for the passed arguments :obj:`mu`\n",
    "        and :obj:`logstd`, or based on latent variables from last encoding.\n",
    "\n",
    "        Args:\n",
    "            mu (Tensor, optional): The latent space for :math:`\\mu`. If set to\n",
    "                :obj:`None`, uses the last computation of :math:`mu`.\n",
    "                (default: :obj:`None`)\n",
    "            logstd (Tensor, optional): The latent space for\n",
    "                :math:`\\log\\sigma`.  If set to :obj:`None`, uses the last\n",
    "                computation of :math:`\\log\\sigma^2`.(default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        mu = self.__mu__ if mu is None else mu\n",
    "        logstd = self.__logstd__ if logstd is None else logstd.clamp(\n",
    "            max=MAX_LOGSTD)\n",
    "        return -0.5 * torch.mean(\n",
    "            torch.sum(1 + 2 * logstd - mu**2 - logstd.exp()**2, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8131, grad_fn=<DivBackward0>)\n",
      "tensor(1.8844, grad_fn=<DivBackward0>)\n",
      "tensor(1.8504, grad_fn=<DivBackward0>)\n",
      "tensor(1.7094, grad_fn=<DivBackward0>)\n",
      "tensor(1.6123, grad_fn=<DivBackward0>)\n",
      "tensor(1.6180, grad_fn=<DivBackward0>)\n",
      "tensor(1.6351, grad_fn=<DivBackward0>)\n",
      "tensor(1.5782, grad_fn=<DivBackward0>)\n",
      "tensor(1.5146, grad_fn=<DivBackward0>)\n",
      "tensor(1.4051, grad_fn=<DivBackward0>)\n",
      "tensor(1.4838, grad_fn=<DivBackward0>)\n",
      "tensor(1.4820, grad_fn=<DivBackward0>)\n",
      "tensor(1.5118, grad_fn=<DivBackward0>)\n",
      "tensor(1.5128, grad_fn=<DivBackward0>)\n",
      "tensor(1.4582, grad_fn=<DivBackward0>)\n",
      "tensor(1.4875, grad_fn=<DivBackward0>)\n",
      "tensor(1.4281, grad_fn=<DivBackward0>)\n",
      "tensor(1.4418, grad_fn=<DivBackward0>)\n",
      "tensor(1.4437, grad_fn=<DivBackward0>)\n",
      "tensor(1.4533, grad_fn=<DivBackward0>)\n",
      "tensor(1.4680, grad_fn=<DivBackward0>)\n",
      "tensor(1.4644, grad_fn=<DivBackward0>)\n",
      "tensor(1.4234, grad_fn=<DivBackward0>)\n",
      "tensor(1.4478, grad_fn=<DivBackward0>)\n",
      "tensor(1.4374, grad_fn=<DivBackward0>)\n",
      "tensor(1.4138, grad_fn=<DivBackward0>)\n",
      "tensor(1.4484, grad_fn=<DivBackward0>)\n",
      "tensor(1.4266, grad_fn=<DivBackward0>)\n",
      "tensor(1.3991, grad_fn=<DivBackward0>)\n",
      "tensor(1.4163, grad_fn=<DivBackward0>)\n",
      "tensor(1.4240, grad_fn=<DivBackward0>)\n",
      "tensor(1.4618, grad_fn=<DivBackward0>)\n",
      "tensor(1.4212, grad_fn=<DivBackward0>)\n",
      "tensor(1.4083, grad_fn=<DivBackward0>)\n",
      "tensor(1.4419, grad_fn=<DivBackward0>)\n",
      "tensor(1.4151, grad_fn=<DivBackward0>)\n",
      "tensor(1.4802, grad_fn=<DivBackward0>)\n",
      "tensor(1.4582, grad_fn=<DivBackward0>)\n",
      "tensor(1.4429, grad_fn=<DivBackward0>)\n",
      "tensor(1.4821, grad_fn=<DivBackward0>)\n",
      "tensor(1.4362, grad_fn=<DivBackward0>)\n",
      "tensor(1.4650, grad_fn=<DivBackward0>)\n",
      "tensor(1.4619, grad_fn=<DivBackward0>)\n",
      "tensor(1.4815, grad_fn=<DivBackward0>)\n",
      "tensor(1.4016, grad_fn=<DivBackward0>)\n",
      "tensor(1.4117, grad_fn=<DivBackward0>)\n",
      "tensor(1.4557, grad_fn=<DivBackward0>)\n",
      "tensor(1.4290, grad_fn=<DivBackward0>)\n",
      "tensor(1.4476, grad_fn=<DivBackward0>)\n",
      "tensor(1.4758, grad_fn=<DivBackward0>)\n",
      "tensor(1.4462, grad_fn=<DivBackward0>)\n",
      "tensor(1.4333, grad_fn=<DivBackward0>)\n",
      "tensor(1.3950, grad_fn=<DivBackward0>)\n",
      "tensor(1.4071, grad_fn=<DivBackward0>)\n",
      "tensor(1.4545, grad_fn=<DivBackward0>)\n",
      "tensor(1.4369, grad_fn=<DivBackward0>)\n",
      "tensor(1.4049, grad_fn=<DivBackward0>)\n",
      "tensor(1.4702, grad_fn=<DivBackward0>)\n",
      "tensor(1.4525, grad_fn=<DivBackward0>)\n",
      "tensor(1.4378, grad_fn=<DivBackward0>)\n",
      "tensor(1.4413, grad_fn=<DivBackward0>)\n",
      "tensor(1.4285, grad_fn=<DivBackward0>)\n",
      "tensor(1.4525, grad_fn=<DivBackward0>)\n",
      "tensor(1.4521, grad_fn=<DivBackward0>)\n",
      "tensor(1.4737, grad_fn=<DivBackward0>)\n",
      "tensor(1.4313, grad_fn=<DivBackward0>)\n",
      "tensor(1.4250, grad_fn=<DivBackward0>)\n",
      "tensor(1.4318, grad_fn=<DivBackward0>)\n",
      "tensor(1.4350, grad_fn=<DivBackward0>)\n",
      "tensor(1.4372, grad_fn=<DivBackward0>)\n",
      "tensor(1.4893, grad_fn=<DivBackward0>)\n",
      "tensor(1.4590, grad_fn=<DivBackward0>)\n",
      "tensor(1.4437, grad_fn=<DivBackward0>)\n",
      "tensor(1.3848, grad_fn=<DivBackward0>)\n",
      "tensor(1.4632, grad_fn=<DivBackward0>)\n",
      "tensor(1.3958, grad_fn=<DivBackward0>)\n",
      "tensor(1.4107, grad_fn=<DivBackward0>)\n",
      "tensor(1.3773, grad_fn=<DivBackward0>)\n",
      "tensor(1.4232, grad_fn=<DivBackward0>)\n",
      "tensor(1.4327, grad_fn=<DivBackward0>)\n",
      "tensor(1.4767, grad_fn=<DivBackward0>)\n",
      "tensor(1.3870, grad_fn=<DivBackward0>)\n",
      "tensor(1.4509, grad_fn=<DivBackward0>)\n",
      "tensor(1.4289, grad_fn=<DivBackward0>)\n",
      "tensor(1.4692, grad_fn=<DivBackward0>)\n",
      "tensor(1.4254, grad_fn=<DivBackward0>)\n",
      "tensor(1.4171, grad_fn=<DivBackward0>)\n",
      "tensor(1.3824, grad_fn=<DivBackward0>)\n",
      "tensor(1.4323, grad_fn=<DivBackward0>)\n",
      "tensor(1.4275, grad_fn=<DivBackward0>)\n",
      "tensor(1.4021, grad_fn=<DivBackward0>)\n",
      "tensor(1.4239, grad_fn=<DivBackward0>)\n",
      "tensor(1.4360, grad_fn=<DivBackward0>)\n",
      "tensor(1.4330, grad_fn=<DivBackward0>)\n",
      "tensor(1.3828, grad_fn=<DivBackward0>)\n",
      "tensor(1.3919, grad_fn=<DivBackward0>)\n",
      "tensor(1.3909, grad_fn=<DivBackward0>)\n",
      "tensor(1.3341, grad_fn=<DivBackward0>)\n",
      "tensor(1.3588, grad_fn=<DivBackward0>)\n",
      "tensor(1.3196, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.autograd.grad_mode import F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "\n",
    "from torch_geometric.nn import VGAE\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "class SimuData():\n",
    "    \"\"\"Simulate graph data\"\"\"\n",
    "    def __init__(self, n_node=10, n_graph=30):\n",
    "        self.n_node = n_node\n",
    "        self.n_graph = n_graph\n",
    "\n",
    "    def simu_adj_wgh(self):\n",
    "        adj_list = []\n",
    "\n",
    "        for i in range(self.n_graph):\n",
    "            ## zero matrix\n",
    "            A = torch.zeros(self.n_node, self.n_node)\n",
    "\n",
    "            # first five nodes weights: uniform(0,1)A[:5,:5] = W\n",
    "            W = torch.rand(5,5)\n",
    "\n",
    "            ## symmetric\n",
    "            i, j = torch.triu_indices(5, 5)\n",
    "            W[i, j] = W.T[i, j]\n",
    "\n",
    "            A[:5,:5] = W\n",
    "            adj_list.append(A)  \n",
    "\n",
    "        return adj_list\n",
    "\n",
    "    def simu_adj_diag(self):\n",
    "        adj_list = []\n",
    "\n",
    "        for i in range(self.n_graph):\n",
    "            A = torch.eye(self.n_node)\n",
    "            adj_list.append(A)  \n",
    "\n",
    "        return adj_list\n",
    "\n",
    "    def simu_adj_m(self):\n",
    "        \"\"\"generating adjacency matrix\"\"\"\n",
    "        adj_wgh = self.simu_adj_wgh()\n",
    "        #adj_wgh = self.simu_adj_diag()\n",
    "        adj_m_list =[]\n",
    "        for _, adj in enumerate(adj_wgh):\n",
    "            adj[adj>0.5] = 1\n",
    "            adj[adj<0.5] = 0\n",
    "            adj_m_list.append(adj)    \n",
    "        return adj_m_list\n",
    "\n",
    "    def graph_dataset(self):\n",
    "        dataset =[]\n",
    "        simu_adj = self.simu_adj_m()\n",
    "\n",
    "        for _, adj in enumerate(simu_adj):\n",
    "            edge_index_temp = sp.coo_matrix(adj)\n",
    "            indices = np.vstack((edge_index_temp.row, edge_index_temp.col))\n",
    "            edge_index_A = torch.LongTensor(indices) \n",
    "            x = self.get_x_feature()\n",
    "            data = Data(x=x, edge_index=edge_index_A)\n",
    "            dataset.append(data)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def get_x_feature(self):\n",
    "        x = torch.arange(self.n_node)\n",
    "        x_onehot = torch.eye(self.n_node)[x,:] \n",
    "\n",
    "        return torch.FloatTensor(x_onehot)\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True) \n",
    "        self.conv_mu = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "        self.conv_logstd = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8121, grad_fn=<DivBackward0>)\n",
      "tensor(1.8447, grad_fn=<DivBackward0>)\n",
      "tensor(1.8195, grad_fn=<DivBackward0>)\n",
      "tensor(1.6452, grad_fn=<DivBackward0>)\n",
      "tensor(1.7079, grad_fn=<DivBackward0>)\n",
      "tensor(1.5158, grad_fn=<DivBackward0>)\n",
      "tensor(1.6303, grad_fn=<DivBackward0>)\n",
      "tensor(1.5081, grad_fn=<DivBackward0>)\n",
      "tensor(1.5434, grad_fn=<DivBackward0>)\n",
      "tensor(1.5781, grad_fn=<DivBackward0>)\n",
      "tensor(1.4733, grad_fn=<DivBackward0>)\n",
      "tensor(1.5638, grad_fn=<DivBackward0>)\n",
      "tensor(1.4540, grad_fn=<DivBackward0>)\n",
      "tensor(1.5166, grad_fn=<DivBackward0>)\n",
      "tensor(1.4247, grad_fn=<DivBackward0>)\n",
      "tensor(1.5014, grad_fn=<DivBackward0>)\n",
      "tensor(1.4098, grad_fn=<DivBackward0>)\n",
      "tensor(1.4483, grad_fn=<DivBackward0>)\n",
      "tensor(1.5225, grad_fn=<DivBackward0>)\n",
      "tensor(1.4728, grad_fn=<DivBackward0>)\n",
      "tensor(1.5135, grad_fn=<DivBackward0>)\n",
      "tensor(1.4147, grad_fn=<DivBackward0>)\n",
      "tensor(1.3859, grad_fn=<DivBackward0>)\n",
      "tensor(1.4624, grad_fn=<DivBackward0>)\n",
      "tensor(1.4577, grad_fn=<DivBackward0>)\n",
      "tensor(1.4640, grad_fn=<DivBackward0>)\n",
      "tensor(1.4293, grad_fn=<DivBackward0>)\n",
      "tensor(1.4082, grad_fn=<DivBackward0>)\n",
      "tensor(1.3838, grad_fn=<DivBackward0>)\n",
      "tensor(1.4040, grad_fn=<DivBackward0>)\n",
      "tensor(1.4327, grad_fn=<DivBackward0>)\n",
      "tensor(1.4292, grad_fn=<DivBackward0>)\n",
      "tensor(1.4651, grad_fn=<DivBackward0>)\n",
      "tensor(1.4282, grad_fn=<DivBackward0>)\n",
      "tensor(1.4557, grad_fn=<DivBackward0>)\n",
      "tensor(1.3826, grad_fn=<DivBackward0>)\n",
      "tensor(1.4437, grad_fn=<DivBackward0>)\n",
      "tensor(1.3984, grad_fn=<DivBackward0>)\n",
      "tensor(1.4122, grad_fn=<DivBackward0>)\n",
      "tensor(1.4119, grad_fn=<DivBackward0>)\n",
      "tensor(1.4182, grad_fn=<DivBackward0>)\n",
      "tensor(1.4364, grad_fn=<DivBackward0>)\n",
      "tensor(1.4269, grad_fn=<DivBackward0>)\n",
      "tensor(1.4575, grad_fn=<DivBackward0>)\n",
      "tensor(1.3883, grad_fn=<DivBackward0>)\n",
      "tensor(1.4313, grad_fn=<DivBackward0>)\n",
      "tensor(1.4150, grad_fn=<DivBackward0>)\n",
      "tensor(1.4363, grad_fn=<DivBackward0>)\n",
      "tensor(1.4354, grad_fn=<DivBackward0>)\n",
      "tensor(1.4394, grad_fn=<DivBackward0>)\n",
      "tensor(1.4300, grad_fn=<DivBackward0>)\n",
      "tensor(1.3883, grad_fn=<DivBackward0>)\n",
      "tensor(1.3480, grad_fn=<DivBackward0>)\n",
      "tensor(1.4201, grad_fn=<DivBackward0>)\n",
      "tensor(1.4393, grad_fn=<DivBackward0>)\n",
      "tensor(1.3754, grad_fn=<DivBackward0>)\n",
      "tensor(1.3908, grad_fn=<DivBackward0>)\n",
      "tensor(1.4112, grad_fn=<DivBackward0>)\n",
      "tensor(1.3712, grad_fn=<DivBackward0>)\n",
      "tensor(1.4294, grad_fn=<DivBackward0>)\n",
      "tensor(1.4030, grad_fn=<DivBackward0>)\n",
      "tensor(1.4421, grad_fn=<DivBackward0>)\n",
      "tensor(1.4049, grad_fn=<DivBackward0>)\n",
      "tensor(1.4478, grad_fn=<DivBackward0>)\n",
      "tensor(1.4310, grad_fn=<DivBackward0>)\n",
      "tensor(1.3885, grad_fn=<DivBackward0>)\n",
      "tensor(1.4297, grad_fn=<DivBackward0>)\n",
      "tensor(1.4416, grad_fn=<DivBackward0>)\n",
      "tensor(1.4330, grad_fn=<DivBackward0>)\n",
      "tensor(1.4450, grad_fn=<DivBackward0>)\n",
      "tensor(1.3759, grad_fn=<DivBackward0>)\n",
      "tensor(1.3948, grad_fn=<DivBackward0>)\n",
      "tensor(1.3997, grad_fn=<DivBackward0>)\n",
      "tensor(1.3834, grad_fn=<DivBackward0>)\n",
      "tensor(1.3916, grad_fn=<DivBackward0>)\n",
      "tensor(1.4108, grad_fn=<DivBackward0>)\n",
      "tensor(1.3752, grad_fn=<DivBackward0>)\n",
      "tensor(1.3768, grad_fn=<DivBackward0>)\n",
      "tensor(1.4206, grad_fn=<DivBackward0>)\n",
      "tensor(1.3502, grad_fn=<DivBackward0>)\n",
      "tensor(1.3310, grad_fn=<DivBackward0>)\n",
      "tensor(1.4255, grad_fn=<DivBackward0>)\n",
      "tensor(1.3700, grad_fn=<DivBackward0>)\n",
      "tensor(1.4293, grad_fn=<DivBackward0>)\n",
      "tensor(1.3644, grad_fn=<DivBackward0>)\n",
      "tensor(1.3545, grad_fn=<DivBackward0>)\n",
      "tensor(1.3145, grad_fn=<DivBackward0>)\n",
      "tensor(1.3996, grad_fn=<DivBackward0>)\n",
      "tensor(1.3086, grad_fn=<DivBackward0>)\n",
      "tensor(1.2953, grad_fn=<DivBackward0>)\n",
      "tensor(1.3436, grad_fn=<DivBackward0>)\n",
      "tensor(1.3173, grad_fn=<DivBackward0>)\n",
      "tensor(1.2551, grad_fn=<DivBackward0>)\n",
      "tensor(1.2736, grad_fn=<DivBackward0>)\n",
      "tensor(1.2943, grad_fn=<DivBackward0>)\n",
      "tensor(1.2502, grad_fn=<DivBackward0>)\n",
      "tensor(1.3113, grad_fn=<DivBackward0>)\n",
      "tensor(1.2148, grad_fn=<DivBackward0>)\n",
      "tensor(1.2004, grad_fn=<DivBackward0>)\n",
      "tensor(1.2150, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "transform = T.Compose([\n",
    "            T.NormalizeFeatures(),\n",
    "            T.RandomLinkSplit(num_val=0, num_test=0, is_undirected=True,\n",
    "                            split_labels=True, add_negative_train_samples=False),])\n",
    "\n",
    "simu_graph = SimuData()\n",
    "dataset = simu_graph.graph_dataset()\n",
    "n_node = simu_graph.n_node\n",
    "\n",
    "out_channels = 2\n",
    "num_features = n_node\n",
    "\n",
    "model = VGAE(Encoder(num_features, out_channels))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "epochs = 100\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_total = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        train_data, val_data, test_data = transform(dataset[i])\n",
    "        z = model.encode(train_data.x, train_data.edge_index)\n",
    "        loss = model.recon_loss(z, train_data.pos_edge_label_index)\n",
    "        loss = loss + (1 /num_features) * model.kl_loss()\n",
    "        loss_total += loss\n",
    "\n",
    "    loss_avg = loss_total/len(dataset)\n",
    "    loss_avg.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss_avg)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1019, -1.2721],\n",
       "        [-0.1744, -0.8355],\n",
       "        [-1.0146, -0.8393],\n",
       "        [-1.3290, -0.3510],\n",
       "        [-0.8987, -1.2068],\n",
       "        [-0.1725, -0.5932],\n",
       "        [-0.2496, -1.0538],\n",
       "        [-0.7701,  0.7809],\n",
       "        [-0.8559,  0.0955],\n",
       "        [ 0.5977,  0.3632]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode(train_data.x, train_data.edge_index)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b0d0757ee47d7ee28d024a252b5d7dd8b20484f5a8ae99b3e03790cf9a4c886"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
